package main

import (
	"context"
	"fmt"
	"network-actions-aggregator/internal/domain/repository"
	"os"
	"sync"
	"sync/atomic"
	"time"

	"github.com/google/uuid"
	"github.com/joho/godotenv"
	"network-actions-aggregator/internal/domain/entity"
	"network-actions-aggregator/internal/infrastructure/postgres"
	postgresRepo "network-actions-aggregator/internal/infrastructure/repository/postgres"
	"network-actions-aggregator/pkg/logger"
)

// Config –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –Ω–∞–≥—Ä—É–∑–∫–∏
type Config struct {
	// Kafka
	KafkaBrokers []string
	KafkaTopic   string

	// Database
	DBConfig postgres.Config

	// Load generation
	TotalEvents     int // –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–±—ã—Ç–∏–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
	DuplicateEvents int // –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
	UserCount       int // –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
	BatchSize       int // –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Kafka
	WorkersCount    int // –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ—Ä–∫–µ—Ä–æ–≤ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏

	// Verification
	VerifyInterval      time.Duration // –ò–Ω—Ç–µ—Ä–≤–∞–ª –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–±—ã—Ç–∏–π –≤ –ë–î
	VerifyTimeout       time.Duration // –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –∑–∞–ø–∏—Å–∏ –≤ –ë–î
	MaxVerifyIterations int           // –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–æ–≤–µ—Ä–∫–∏
}

// Metrics –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
type Metrics struct {
	eventsSent         atomic.Int64
	eventsVerified     atomic.Int64
	eventsFailed       atomic.Int64
	duplicatesSent     atomic.Int64
	duplicatesRejected atomic.Int64

	startTime time.Time
	endTime   time.Time
	mu        sync.RWMutex
}

func main() {
	if err := run(); err != nil {
		_, _ = fmt.Fprintf(os.Stderr, "error: %v\n", err)
		os.Exit(1)
	}
}

func run() error {
	// –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
	if err := godotenv.Load(); err != nil {
		fmt.Printf("Warning: .env file not found: %v\n", err)
	}

	// –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞
	log := logger.NewLogger()

	// –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
	config := Config{
		KafkaBrokers: []string{
			fmt.Sprintf("%s:%s",
				getEnv("KAFKA_HOST", "localhost"),
				getEnv("KAFKA_PORT", "9092")),
		},
		KafkaTopic: getEnv("KAFKA_TOPIC", "events.telecom"),

		DBConfig: postgres.Config{
			Host:            getEnv("POSTGRES_HOST", "localhost"),
			Port:            getEnv("POSTGRES_PORT", "5432"),
			User:            getEnv("POSTGRES_USER", "app_user"),
			Password:        getEnv("POSTGRES_PASSWORD", "app_password"),
			DBName:          getEnv("POSTGRES_DB", "network_events"),
			SSLMode:         getEnv("POSTGRES_SSLMODE", "disable"),
			MaxOpenConns:    getEnvInt("POSTGRES_MAX_OPEN_CONNS", 10),
			MaxIdleConns:    getEnvInt("POSTGRES_MAX_IDLE_CONNS", 5),
			ConnMaxLifetime: 5 * time.Minute,
			ConnMaxIdleTime: 5 * time.Minute,
		},

		TotalEvents:         getEnvInt("GENERATOR_TOTAL_EVENTS", 100000),
		DuplicateEvents:     getEnvInt("GENERATOR_DUPLICATE_EVENTS", 1000),
		UserCount:           getEnvInt("GENERATOR_USER_COUNT", 1000),
		BatchSize:           getEnvInt("GENERATOR_BATCH_SIZE", 100),
		WorkersCount:        getEnvInt("GENERATOR_WORKERS", 4),
		VerifyInterval:      parseDuration(getEnv("GENERATOR_VERIFY_INTERVAL", "5s"), 5*time.Second),
		VerifyTimeout:       parseDuration(getEnv("GENERATOR_VERIFY_TIMEOUT", "5m"), 5*time.Minute),
		MaxVerifyIterations: getEnvInt("GENERATOR_MAX_VERIFY_ITERATIONS", 60),
	}

	log.Info("=== LOAD GENERATOR CONFIGURATION ===", map[string]interface{}{
		"kafka_brokers":         config.KafkaBrokers,
		"kafka_topic":           config.KafkaTopic,
		"total_events":          config.TotalEvents,
		"duplicate_events":      config.DuplicateEvents,
		"user_count":            config.UserCount,
		"batch_size":            config.BatchSize,
		"workers":               config.WorkersCount,
		"verify_interval":       config.VerifyInterval.String(),
		"verify_timeout":        config.VerifyTimeout.String(),
		"max_verify_iterations": config.MaxVerifyIterations,
	})

	// –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏
	db, err := postgres.NewConnection(config.DBConfig)
	if err != nil {
		return fmt.Errorf("failed to connect to database: %w", err)
	}
	sqlDB, err := db.DB()
	if err != nil {
		return fmt.Errorf("failed to get database instance: %w", err)
	}
	defer func() {
		_ = sqlDB.Close()
	}()

	log.Info("connected to database for verification", map[string]interface{}{
		"host":     config.DBConfig.Host,
		"database": config.DBConfig.DBName,
	})

	// –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
	generator := NewEventGenerator(config.UserCount)
	producer := NewKafkaProducer(config.KafkaBrokers, config.KafkaTopic)
	defer func() {
		_ = producer.Close()
	}()

	eventRepo := postgresRepo.NewEventRepository(db)
	verifier := NewVerifier(eventRepo)

	// –ú–µ—Ç—Ä–∏–∫–∏
	metrics := &Metrics{
		startTime: time.Now(),
	}

	ctx := context.Background()

	// –§–∞–∑–∞ 1: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π
	log.Info("=== PHASE 1: GENERATING UNIQUE EVENTS ===", map[string]interface{}{
		"count": config.TotalEvents,
	})

	uniqueEvents, err := generateAndSendEvents(ctx, log, config, generator, producer, verifier, metrics, config.TotalEvents)
	if err != nil {
		return fmt.Errorf("failed to generate and send unique events: %w", err)
	}

	log.Info("unique events sent to Kafka", map[string]interface{}{
		"sent":   metrics.eventsSent.Load(),
		"failed": metrics.eventsFailed.Load(),
	})

	// –§–∞–∑–∞ 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
	log.Info("=== PHASE 2: GENERATING DUPLICATE EVENTS ===", map[string]interface{}{
		"count": config.DuplicateEvents,
	})

	if err := generateAndSendDuplicates(ctx, log, config, producer, metrics, uniqueEvents, config.DuplicateEvents); err != nil {
		return fmt.Errorf("failed to generate and send duplicates: %w", err)
	}

	log.Info("duplicate events sent to Kafka", map[string]interface{}{
		"sent": metrics.duplicatesSent.Load(),
	})

	// –§–∞–∑–∞ 3: –û–∂–∏–¥–∞–Ω–∏–µ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ –ë–î
	log.Info("=== PHASE 3: WAITING FOR DATABASE PERSISTENCE ===", nil)

	if err := waitForPersistence(ctx, log, config, verifier, metrics); err != nil {
		return fmt.Errorf("failed to verify persistence: %w", err)
	}

	// –§–∞–∑–∞ 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
	log.Info("=== PHASE 4: VERIFYING DUPLICATES WERE REJECTED ===", nil)

	if err := verifyDuplicatesRejected(log, verifier, metrics, config.TotalEvents); err != nil {
		return fmt.Errorf("failed to verify duplicates rejection: %w", err)
	}

	// –§–∞–∑–∞ 5: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–≥—Ä–µ–≥–∞—Ü–∏–π
	log.Info("=== PHASE 5: VERIFYING AGGREGATIONS ===", nil)

	aggregationRepo := postgresRepo.NewAggregationRepository(db)
	if err := verifyAggregations(ctx, log, config, verifier, aggregationRepo, uniqueEvents); err != nil {
		return fmt.Errorf("failed to verify aggregations: %w", err)
	}

	// –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
	metrics.mu.Lock()
	metrics.endTime = time.Now()
	metrics.mu.Unlock()

	printFinalReport(log, metrics, config)

	return nil
}

// generateAndSendEvents –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è
func generateAndSendEvents(
	ctx context.Context,
	log logger.Logger,
	config Config,
	generator *EventGenerator,
	producer *KafkaProducer,
	verifier *Verifier,
	metrics *Metrics,
	totalEvents int,
) ([]*entity.Event, error) {
	allEvents := make([]*entity.Event, 0, totalEvents)
	var allEventsMu sync.Mutex

	eventsChan := make(chan []*entity.Event, config.WorkersCount*2)
	errorsChan := make(chan error, config.WorkersCount)

	var wg sync.WaitGroup

	// –í–æ—Ä–∫–µ—Ä—ã –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Kafka
	for i := 0; i < config.WorkersCount; i++ {
		wg.Add(1)
		workerID := i
		go func() {
			defer wg.Done()

			for events := range eventsChan {
				if err := producer.SendBatch(ctx, events); err != nil {
					log.Error("failed to send batch", map[string]interface{}{
						"worker": workerID,
						"error":  err,
						"size":   len(events),
					})
					metrics.eventsFailed.Add(int64(len(events)))
					errorsChan <- err
					continue
				}

				metrics.eventsSent.Add(int64(len(events)))

				// –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏
				eventIDs := make([]uuid.UUID, len(events))
				for i, event := range events {
					eventIDs[i] = event.EventID
				}
				verifier.TrackSentBatch(eventIDs)
			}
		}()
	}

	// –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–±—ã—Ç–∏–π –±–∞—Ç—á–∞–º–∏
	go func() {
		defer close(eventsChan)

		for i := 0; i < totalEvents; i += config.BatchSize {
			batchSize := config.BatchSize
			if i+batchSize > totalEvents {
				batchSize = totalEvents - i
			}

			events := generator.GenerateBatch(batchSize)

			allEventsMu.Lock()
			allEvents = append(allEvents, events...)
			allEventsMu.Unlock()

			select {
			case eventsChan <- events:
			case <-ctx.Done():
				return
			}

			// –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
			if (i+batchSize)%10000 == 0 || i+batchSize >= totalEvents {
				log.Info("generation progress", map[string]interface{}{
					"generated": i + batchSize,
					"total":     totalEvents,
					"progress":  fmt.Sprintf("%.1f%%", float64(i+batchSize)/float64(totalEvents)*100),
				})
			}
		}
	}()

	// –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ—Ç–ø—Ä–∞–≤–∫–∏
	wg.Wait()
	close(errorsChan)

	// –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—à–∏–±–∫–∏
	if len(errorsChan) > 0 {
		return allEvents, fmt.Errorf("encountered %d errors during sending", len(errorsChan))
	}

	return allEvents, nil
}

// generateAndSendDuplicates –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å–æ–±—ã—Ç–∏–π
func generateAndSendDuplicates(
	ctx context.Context,
	log logger.Logger,
	config Config,
	producer *KafkaProducer,
	metrics *Metrics,
	originalEvents []*entity.Event,
	duplicateCount int,
) error {
	if len(originalEvents) == 0 {
		return fmt.Errorf("no original events to duplicate")
	}

	eventsChan := make(chan []*entity.Event, config.WorkersCount*2)
	errorsChan := make(chan error, config.WorkersCount)

	var wg sync.WaitGroup

	// –í–æ—Ä–∫–µ—Ä—ã –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
	for i := 0; i < config.WorkersCount; i++ {
		wg.Add(1)
		workerID := i
		go func() {
			defer wg.Done()

			for events := range eventsChan {
				if err := producer.SendBatch(ctx, events); err != nil {
					log.Error("failed to send duplicate batch", map[string]interface{}{
						"worker": workerID,
						"error":  err,
						"size":   len(events),
					})
					errorsChan <- err
					continue
				}

				metrics.duplicatesSent.Add(int64(len(events)))
			}
		}()
	}

	// –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
	go func() {
		defer close(eventsChan)

		for i := 0; i < duplicateCount; i += config.BatchSize {
			batchSize := config.BatchSize
			if i+batchSize > duplicateCount {
				batchSize = duplicateCount - i
			}

			batch := make([]*entity.Event, batchSize)
			for j := 0; j < batchSize; j++ {
				// –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω–æ–µ —Å–æ–±—ã—Ç–∏–µ –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö
				idx := (i + j) % len(originalEvents)
				batch[j] = originalEvents[idx]
			}

			select {
			case eventsChan <- batch:
			case <-ctx.Done():
				return
			}
		}
	}()

	wg.Wait()
	close(errorsChan)

	if len(errorsChan) > 0 {
		return fmt.Errorf("encountered %d errors during sending duplicates", len(errorsChan))
	}

	return nil
}

// waitForPersistence –æ–∂–∏–¥–∞–µ—Ç –∑–∞–ø–∏—Å–∏ –≤—Å–µ—Ö —Å–æ–±—ã—Ç–∏–π –≤ –ë–î
func waitForPersistence(
	ctx context.Context,
	log logger.Logger,
	config Config,
	verifier *Verifier,
	metrics *Metrics,
) error {
	log.Info("waiting for all events to be persisted in database...", nil)

	ticker := time.NewTicker(config.VerifyInterval)
	defer ticker.Stop()

	timeout := time.After(config.VerifyTimeout)
	iteration := 0

	for {
		select {
		case <-timeout:
			stats := verifier.Stats()
			return fmt.Errorf("timeout waiting for persistence: verified %d/%d events",
				stats.Verified, stats.TotalSent)

		case <-ticker.C:
			iteration++

			if err := verifier.VerifyAll(ctx); err != nil {
				log.Error("verification failed", map[string]interface{}{
					"error":     err,
					"iteration": iteration,
				})
				continue
			}

			stats := verifier.Stats()
			metrics.eventsVerified.Store(int64(stats.Verified))

			log.Info("verification progress", map[string]interface{}{
				"iteration":    iteration,
				"sent":         stats.TotalSent,
				"verified":     stats.Verified,
				"missing":      stats.Missing,
				"pending":      stats.Pending,
				"success_rate": fmt.Sprintf("%.2f%%", stats.SuccessRate),
				"avg_latency":  stats.AvgLatency.Round(time.Millisecond).String(),
			})

			// –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤—Å–µ –ª–∏ —Å–æ–±—ã—Ç–∏—è –∑–∞–ø–∏—Å–∞–Ω—ã
			if stats.Verified == stats.TotalSent {
				log.Info("‚úì ALL EVENTS SUCCESSFULLY PERSISTED!", map[string]interface{}{
					"total":       stats.TotalSent,
					"iterations":  iteration,
					"avg_latency": stats.AvgLatency.Round(time.Millisecond).String(),
				})
				return nil
			}

			// –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
			if iteration >= config.MaxVerifyIterations {
				return fmt.Errorf("max verification iterations reached: verified %d/%d events",
					stats.Verified, stats.TotalSent)
			}
		}
	}
}

// verifyDuplicatesRejected –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –¥—É–±–ª–∏–∫–∞—Ç—ã –ù–ï –∑–∞–ø–∏—Å–∞–ª–∏—Å—å –≤ –ë–î
func verifyDuplicatesRejected(
	log logger.Logger,
	verifier *Verifier,
	metrics *Metrics,
	expectedCount int,
) error {
	stats := verifier.Stats()

	// –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π –≤ –ë–î –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ä–∞–≤–Ω–æ expectedCount
	if stats.Verified != expectedCount {
		return fmt.Errorf("unexpected event count in DB: expected %d, got %d",
			expectedCount, stats.Verified)
	}

	duplicatesRejected := metrics.duplicatesSent.Load()
	metrics.duplicatesRejected.Store(duplicatesRejected)

	log.Info("‚úì DUPLICATES VERIFICATION PASSED!", map[string]interface{}{
		"duplicates_sent":     duplicatesRejected,
		"duplicates_rejected": duplicatesRejected,
		"unique_events_in_db": stats.Verified,
	})

	return nil
}

// verifyAggregations –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∞–≥—Ä–µ–≥–∞—Ü–∏–π
func verifyAggregations(
	ctx context.Context,
	log logger.Logger,
	config Config,
	verifier *Verifier,
	aggregationRepo repository.AggregationRepository,
	events []*entity.Event,
) error {
	log.Info("waiting for aggregations to be computed...", map[string]interface{}{
		"events_count": len(events),
	})

	// –ñ–¥–µ–º –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è, —á—Ç–æ–±—ã –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä –æ–±—Ä–∞–±–æ—Ç–∞–ª —Å–æ–±—ã—Ç–∏—è
	log.Info("waiting 30 seconds for aggregator to process events...", nil)
	time.Sleep(30 * time.Second)

	log.Info("verifying aggregations...", nil)

	if err := verifier.VerifyAggregations(ctx, aggregationRepo, events); err != nil {
		log.Error("aggregation verification failed", map[string]interface{}{
			"error": err,
		})
		return err
	}

	log.Info("‚úì AGGREGATIONS VERIFICATION PASSED!", nil)
	return nil
}

// printFinalReport –≤—ã–≤–æ–¥–∏—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏
func printFinalReport(log logger.Logger, metrics *Metrics, config Config) {
	totalDuration := metrics.endTime.Sub(metrics.startTime)
	sent := metrics.eventsSent.Load()
	failed := metrics.eventsFailed.Load()
	verified := metrics.eventsVerified.Load()
	duplicatesSent := metrics.duplicatesSent.Load()
	duplicatesRejected := metrics.duplicatesRejected.Load()

	eventsPerSec := float64(verified) / totalDuration.Seconds()

	log.Info("", nil)
	log.Info("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó", nil)
	log.Info("‚ïë           FINAL PERFORMANCE REPORT                    ‚ïë", nil)
	log.Info("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù", nil)
	log.Info("", nil)
	log.Info("üìä GENERATION STATS:", map[string]interface{}{
		"total_duration":   totalDuration.Round(time.Millisecond).String(),
		"unique_generated": config.TotalEvents,
		"unique_sent":      sent,
		"unique_failed":    failed,
		"unique_verified":  verified,
		"duplicates_sent":  duplicatesSent,
		"duplicates_in_db": 0, // –î—É–±–ª–∏–∫–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Ç–∫–ª–æ–Ω–µ–Ω—ã
	})
	log.Info("", nil)
	log.Info("‚úÖ VERIFICATION RESULTS:", map[string]interface{}{
		"expected_in_db":         config.TotalEvents,
		"actual_in_db":           verified,
		"all_unique_saved":       verified == int64(config.TotalEvents),
		"duplicates_rejected":    duplicatesRejected,
		"all_duplicates_blocked": duplicatesRejected == duplicatesSent,
	})
	log.Info("", nil)
	log.Info("‚ö° PERFORMANCE METRICS:", map[string]interface{}{
		"events_per_second":    fmt.Sprintf("%.2f", eventsPerSec),
		"avg_time_per_event":   fmt.Sprintf("%.3f ms", 1000.0/eventsPerSec),
		"total_events_handled": sent + duplicatesSent,
	})
	log.Info("", nil)

	// –§–∏–Ω–∞–ª—å–Ω—ã–π –≤–µ—Ä–¥–∏–∫—Ç
	success := verified == int64(config.TotalEvents) && duplicatesRejected == duplicatesSent && failed == 0
	if success {
		log.Info("üéâ TEST RESULT: SUCCESS! All requirements met.", nil)
	} else {
		log.Info("‚ùå TEST RESULT: FAILED! Some requirements not met.", nil)
	}
	log.Info("", nil)
}

func getEnv(key, defaultValue string) string {
	if value := os.Getenv(key); value != "" {
		return value
	}
	return defaultValue
}

func getEnvInt(key string, defaultValue int) int {
	if value := os.Getenv(key); value != "" {
		var result int
		if _, err := fmt.Sscanf(value, "%d", &result); err == nil {
			return result
		}
	}
	return defaultValue
}

func parseDuration(value string, defaultValue time.Duration) time.Duration {
	if d, err := time.ParseDuration(value); err == nil {
		return d
	}
	return defaultValue
}
