# Что делает система

1. Принимает поток событий от мобильной сети: исходящий/входящий звонок, отправка/получение смс, интернет-сессии с объёмом и длительностью.
2. Ничего не теряет: даже если что-то падает, данные доходят.
3. Даёт два вида ответов:

    * детализация по одному пользователю за период в формате json;
    * агрегированные цифры по всем пользователям по дням и месяцам (сколько смс, звонков, интернет-сессий и т. п.).

# Общая картинка

```
[Генератор/источник событий]
           │
           ▼
      [Redpanda]
           │
    (consumer-группа)
           │
           ▼
   [Сервис приёма событий]
   - пакетные вставки
   - дедубликация
   - запись в БД
           │
           ├──────────────► [Таблица событий (Postgres, партиции)]
           │
           └──────────────► [Сервис агрегации] ──► [Таблицы агрегатов день/месяц]
                                        │
                                        └──► [Инвалидатор кеша]
                                                 │
                                                 ▼
                                             [Redis]
                                                 ▲
                                                 │
                                         [API сервис]
                                   (gRPC + HTTP шлюз)
```

# Основные компоненты

## 1) Очередь событий (Redpanda)

* Зачем: чтобы не потерять события, уметь читать их параллельно, выдерживать пики.
* Топик: `events.telecom`
* Ключ сообщения: `user_id`
  Это позволит одному пользователю держать события в порядке, а между пользователями читать параллельно.
* Формат сообщения (json, чтобы было проще):

```json
{
  "event_id": "uuid",
  "source_id": "provider-1",
  "user_id": "123456789",
  "type": "CALL_OUT|CALL_IN|SMS_SENT|SMS_RECEIVED|DATA",
  "started_at": "2025-10-20T12:34:56Z",
  "ended_at": "2025-10-20T12:56:00Z",
  "duration_sec": 127,
  "bytes_up": 12345,
  "bytes_down": 67890,
  "meta": {"cell":"A1","imei":"..."}
}
```

* Гарантия доставки: как минимум один раз.
  На нашей стороне делаем защиту от дублей.

## 2) Сервис приёма событий (ingestor)

* Язык: Go.
* Что делает:

    * читает сообщения батчами;
    * проверяет, не дубликат ли (по `event_id` + `source_id`, или по хэшу полезной части);
    * пишет в базу пакетной вставкой;
    * не падает от ошибок: ретраи с увеличением паузы, подтверждение чтения только после успешной записи.
* Параллельность:

    * Пул воркеров (например 8–32).
    * Очередь задач между чтением и записью.
* Порядок по одному пользователю сохраняется «почти всегда» благодаря ключу партиции и обработке одним воркером в моменте. Для детальной выборки это достаточно.

## 3) База данных (PostgreSQL)

* Почему не ClickHouse — уже обсудили: нам важны частые вставки, транзакции, простота старта.
* Основная таблица: `events_raw`
* Партиционирование по месяцу по полю `started_at`.
  Пример имён партиций: `events_raw_2025_10`, `events_raw_2025_11` …
* Индексы:

    * `(user_id, started_at)` — для детализации;
    * `(type, started_at)` — для агрегирования.
* Вставка:

    * пакетами по 500–5000 строк (подберёшь по профилю);
    * `INSERT ... ON CONFLICT DO NOTHING` для защиты от дублей по уникальному индексу.

Минимальная схема (упрощённая):

## 4) Сервис агрегации

* Отдельный процесс (Go), тоже читает из Redpanda.
* На каждое событие обновляет строку агрегации за нужный день.
  Делает upsert:

    * если строки нет — вставляет с начальными суммами;
    * если есть — увеличивает поля.
* Для месяцей есть два пути:

    * пересчитывать месяц «на лету»: когда наступает новый день, обновлять соответствующий месяц;
    * раз в час запускать «догонку»: пробегать дни прошедшего месяца и сводить в месяц.
* Почему так: инкрементальные апдейты маленькие и быстрые; «догонка» нужна, чтобы привести данные к идеалу при задержках.

Пример upsert по дню:

```sql
insert into agg_day (day, type, count_events, sum_duration, sum_bytes_up, sum_bytes_down)
values ($1, $2, 1, $3, $4, $5)
on conflict (day, type)
do update set
  count_events = agg_day.count_events + 1,
  sum_duration = agg_day.sum_duration + excluded.sum_duration,
  sum_bytes_up = agg_day.sum_bytes_up + excluded.sum_bytes_up,
  sum_bytes_down = agg_day.sum_bytes_down + excluded.sum_bytes_down;
```

## 5) Кеш (Redis)

* Для ускорения **детализации по пользователю** и **агрегатов**.
* Ключи:

    * детализация: `user:{id}:detail:{from}:{to}:v1`
    * агрегаты день: `agg:day:{from}:{to}:v1`
    * агрегаты месяц: `agg:month:{from}:{to}:v1`
* Время жизни:

    * детализация: 5–30 минут (подберёшь, зависит от нагрузки);
    * агрегаты: 1–6 часов (дольше меняются редко).
* Защита от «набега» запросов: один поток прогревает кеш (singleflight), остальные ждут готовый ответ.
* Инвалидация:

    * Когда сервис агрегации догоняет старые дни/месяцы, он удаляет соответствующие ключи.

## 6) API сервис

* Интерфейсы: gRPC и поверх него HTTP шлюз (grpc-gateway).
* Методы:

    * `GetUserDetail(user_id, from, to)` — список событий в json (с пагинацией);
    * `GetGlobalAgg(granularity, from, to)` — суммы по типам событий за дни или месяцы.
* Порядок ответа:

    * сначала смотрим Redis;
    * если пусто — читаем из базы, собираем ответ, кладём в Redis, отдаём клиенту.

Упрощённый формат ответов:

**Детализация:**

```json
{
  "user_id": "123",
  "from": "2025-10-01T00:00:00Z",
  "to":   "2025-10-31T23:59:59Z",
  "items": [
    {
      "type": "CALL_OUT",
      "started_at": "2025-10-20T12:34:56Z",
      "duration_sec": 127
    },
    {
      "type": "DATA",
      "started_at": "2025-10-20T14:10:00Z",
      "bytes_up": 10000,
      "bytes_down": 20000
    }
  ],
  "next_cursor": "opaque-string"
}
```

**Агрегаты:**

```json
{
  "granularity": "DAY",
  "from": "2025-10-01",
  "to":   "2025-10-31",
  "data": [
    {
      "date": "2025-10-01",
      "type": "SMS_SENT",
      "count_events": 1000,
      "sum_duration": 0,
      "sum_bytes_up": 0,
      "sum_bytes_down": 0
    }
  ]
}
```

## 7) Профилирование и наблюдаемость

* Включаем `pprof` на каждом сервисе на своём порту.
  Снимаем профили CPU и памяти под нагрузкой, ищем «узкие места».
* Логи структурные (json): уровень, время, сервис, трасс-идентификатор.
* Простейшие метрики в логах: сколько событий в секунду, сколько ошибок, средняя задержка ответов.

## 8) Надёжность и «не теряем данные»

* Очередь (Redpanda) — хранит сообщения. Мы подтверждаем чтение только после успешной записи в базу.
* База — вставки в транзакции, при ошибке откатываем.
* Дедубликация — уникальный ключ `(source_id, event_id)` или хэш полезной части.
  Это спасёт от повторной доставки из очереди.
* Завершение работы:

    * аккуратно: закрываем приём, ждём пока воркеры закончат, сохраняем оффсет, выходим.

## 9) Многопоточность

* В читателе из Redpanda — несколько потоков на партиции.
* В обработке — пул воркеров: один воркер обрабатывает один пакет, вставляет в базу.
* В API — для чтения из базы используем соединения из пула.
* Везде следим за контекстами, чтобы не висели горутины. В тестах используем проверку утечек.

## 10) Тестирование

* Юнит-тесты:

    * парсинг событий;
    * дедубликация;
    * составление sql для пакетных вставок;
    * логика кеша (ключи, ttl, инвалидация).
* Интеграционные тесты:

    * поднимаем docker compose (Redpanda, Postgres, Redis);
    * прогоняем сценарии: положительный поток, дубликаты, задержки.
* API-тесты:

    * пустой ответ, много страниц, неверные параметры, большие интервалы.
* Нагрузочные черновые:

    * генератор шлёт 50–200 тысяч событий в минуту (сколько вытянет локальный ноут);
    * меряем задержки вставок и ответов API.

## 11) Безопасность (базово, чтобы не мешало)

* Ограничения по размеру запроса и окну времени (например, детализация не больше 31 дня за раз).
* Простая защита от частых запросов: токен-бакет на IP или на ключ клиента.
* Секреты (пароли, ключи) — через переменные окружения, не в коде.

## 12) Развёртывание

* Локально: `docker compose up` — поднимает Redpanda, Postgres, Redis, и три наших сервиса.
* Конфигурация через `.env`:

    * строки подключения,
    * размеры батчей,
    * количество воркеров,
    * ttl кеша.
* Для «релиза 0.1» хватит одного узла. Потом можно разнести по контейнерам/верам отдельно.

# Поток данных «от и до»

1. Источник положил сообщение в Redpanda.
2. Наш сервис приёма вытащил пакет, проверил дубли, записал пакет в Postgres, подтвердил чтение.
3. Сервис агрегации тоже читает поток и обновляет `agg_day` (и в итоге `agg_month`).
4. Клиент вызывает API:

    * сервис смотрит Redis → если есть — отдаёт;
    * если нет — читает из Postgres (детализацию или суммы), складывает в Redis, отдаёт клиенту.
5. Когда «догонка» или инкремент изменили агрегаты — инвалидатор удаляет устаревшие ключи в Redis.

# Как отвечаем быстро

* Детализация:
  читаем по ключу `(user_id, started_at)` с пагинацией по курсору (а не OFFSET), чтобы не тормозило на больших таблицах.
* Агрегаты:
  таблицы уже «сплющены», поэтому это быстрые простые чтения по дате.

# Что можно улучшать позже

* Вынести события в «сырой» слой в файловое хранилище (объектное), а в Postgres хранить только «рабочий» слой — когда данных станет очень много.
* Подключить ClickHouse параллельно для очень длинной ретроспективы и тяжёлых отчётов.
* Добавить обычные метрики (Prometheus + Grafana).
* Сделать прокси-кеш на стороне http-шлюза для публичных агрегатов.


# Ключевые решения кратко

* **Очередь** нужна, чтобы переживать пики и не терять данные.
* **Postgres** выбран, потому что у нас много частых вставок, транзакции и простой старт.
* **Партиции по месяцу** — чтобы таблица не распухала одним куском, и чтобы запросы по датам летали.
* **Агрегаторы** считаются «на лету», месяц сводим до кучи планово.
* **Redis** ускоряет чтение, а инвалидатор поддерживает актуальность.
* **Идемпотентность** плюс **подтверждение чтения только после записи** — не теряем события.
* **Профилирование** обязательно: включаем `pprof`, фиксируем цифры до/после.

# Главный промпт для нейронки
если делаешь какую то сложную (или не очень) логику, оставляй такие todo чтобы я реализовывал сам. 

вот пример
// ConsumeEvents читает события из Kafka и отправляет в канал
// Блокирует до отмены контекста
func (c *ConsumerV2) ConsumeEvents(ctx context.Context, eventsChan chan<- *entity.Event) error {
defer close(eventsChan)

	c.logger.Info("starting kafka consumer v2")

	// TODO: реализуй бесконечный цикл чтения
	// 1. Проверяй ctx.Done() через select
	// 2. Читай сообщение через FetchMessage
	// 3. Парси JSON в entity.Event
	// 4. Отправь в eventsChan (с проверкой ctx.Done!)
	// 5. Закоммить offset через CommitMessages
	// 6. Обработай ошибки на каждом этапе
	
	

	return nil
}

